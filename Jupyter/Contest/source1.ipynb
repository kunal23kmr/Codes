{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c358c056",
   "metadata": {},
   "source": [
    "Making the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a67ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vaex\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b914546",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.read_csv('Doceree-HCP_Train.csv', encoding='latin-1')\n",
    "test =pd.read_csv('Doceree-HCP-Test.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f485943e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=['Desktop' 'Desktop' 'Desktop' ... 'Desktop' 'Mobile' 'Mobile'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m      5\u001b[0m     \u001b[39mif\u001b[39;00m data[column]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m      6\u001b[0m         \u001b[39m# Encode the string data to numerical values\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m         data[column] \u001b[39m=\u001b[39m label_encoder\u001b[39m.\u001b[39;49mfit_transform(data[column])\n\u001b[0;32m      9\u001b[0m \u001b[39m# The Same thing for test_dataset\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m test\u001b[39m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mc:\\Users\\kunal\\miniconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\kunal\\miniconda3\\lib\\site-packages\\sklearn\\base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    877\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    879\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    881\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\kunal\\miniconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1303\u001b[0m, in \u001b[0;36mOrdinalEncoder.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   1297\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39munknown_value should only be set when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1298\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandle_unknown is \u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_encoded_value\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munknown_value\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1300\u001b[0m     )\n\u001b[0;32m   1302\u001b[0m \u001b[39m# `_fit` will only raise an error when `self.handle_unknown=\"error\"`\u001b[39;00m\n\u001b[1;32m-> 1303\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown, force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1305\u001b[0m cardinalities \u001b[39m=\u001b[39m [\u001b[39mlen\u001b[39m(categories) \u001b[39mfor\u001b[39;00m categories \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories_]\n\u001b[0;32m   1307\u001b[0m \u001b[39m# stores the missing indices per category\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kunal\\miniconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:74\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[1;34m(self, X, handle_unknown, force_all_finite, return_counts)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_names(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 74\u001b[0m X_list, n_samples, n_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X(\n\u001b[0;32m     75\u001b[0m     X, force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite\n\u001b[0;32m     76\u001b[0m )\n\u001b[0;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_ \u001b[39m=\u001b[39m n_features\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\kunal\\miniconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:46\u001b[0m, in \u001b[0;36m_BaseEncoder._check_X\u001b[1;34m(self, X, force_all_finite)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39mPerform custom check_array:\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m- convert list of strings to object dtype\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \n\u001b[0;32m     43\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mgetattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[0;32m     45\u001b[0m     \u001b[39m# if not a dataframe, do normal check_array validation\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     X_temp \u001b[39m=\u001b[39m check_array(X, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite)\n\u001b[0;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(X_temp\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39mstr_):\n\u001b[0;32m     48\u001b[0m         X \u001b[39m=\u001b[39m check_array(X, dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m, force_all_finite\u001b[39m=\u001b[39mforce_all_finite)\n",
      "File \u001b[1;32mc:\\Users\\kunal\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:902\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    903\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    905\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    906\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    907\u001b[0m         )\n\u001b[0;32m    909\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    910\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    911\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    912\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    913\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['Desktop' 'Desktop' 'Desktop' ... 'Desktop' 'Mobile' 'Mobile'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Assuming 'data' is pandas DataFrame containing string columns\n",
    "label_encoder = OrdinalEncoder()\n",
    "\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'object':\n",
    "        # Encode the string data to numerical values\n",
    "        data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "# The Same thing for test_dataset\n",
    "for column in test.columns:\n",
    "    if test[column].dtype == 'object':\n",
    "        test[column] = label_encoder.fit_transform(test[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaex_data=vaex.from_pandas(data)\n",
    "vaex_test=vaex.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5784611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaex_data.export_hdf5('Doceree-HCP_Train.hdf5')\n",
    "vaex_test.export_hdf5('Doceree-HCP-Test.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f8df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # imputing\n",
    "# imputer = SimpleImputer(strategy='mean')\n",
    "# # txo_imputer = SimpleImputer(strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aead66",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaex_df=vaex.open('Doceree-HCP_Train.hdf5')\n",
    "# Iterate over each column in the DataFrame\n",
    "for column in vaex_df.columns:\n",
    "    # Fill missing values with 0 for the current column\n",
    "    vaex_df[column].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c560a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaex_df = vaex_df.shuffle(random_state=42)  # Shuffle the DataFrame with a fixed random state\n",
    "df_train,df_test = vaex_df.ml.train_test_split(test_size=0.2)  # Split into train and test sets\n",
    "\n",
    "# df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b9df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['DEVICETYPE', 'PLATFORM_ID', 'BIDREQUESTIP', 'USERPLATFORMUID', 'USERCITY', 'USERZIPCODE', 'USERAGENT', 'PLATFORMTYPE', 'CHANNELTYPE', 'URL', 'KEYWORDS']\n",
    "target='IS_HCP'\n",
    "txo_features = ['DEVICETYPE', 'PLATFORM_ID', 'BIDREQUESTIP', 'USERPLATFORMUID', 'USERCITY', 'USERZIPCODE', 'USERAGENT', 'PLATFORMTYPE', 'CHANNELTYPE', 'URL', 'KEYWORDS']\n",
    "txo_target='TAXONOMY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf3ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RandomForestRegressor model\n",
    "model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38771eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with a specific value (e.g., 0)\n",
    "df_train = df_train.fillna(0)\n",
    "df_test = df_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features and target arrays from the DataFrames\n",
    "X_train = df_train[features].values\n",
    "# X_train.shapes\n",
    "# print(X_train)\n",
    "y_train = df_train[target].values\n",
    "y_train = y_train.ravel()\n",
    "txo_train = y_train #to use in futuer\n",
    "\n",
    "X_test = df_test[features].values\n",
    "y_test = df_test[target].values\n",
    "y_test = y_test.ravel()\n",
    "txo_test = y_test #to use in futuer\n",
    "\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e36131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # impute NaN values\n",
    "# X_train[np.isnan(X_train)] = 0\n",
    "\n",
    "# y_train = pd.Series(y_train)\n",
    "# # Fill missing values in y_train with 0\n",
    "# y_train = y_train.fillna(0)\n",
    "\n",
    "# print(X_train.shape , y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f13f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "# X_test[np.isnan(X_test)] = 0\n",
    "predictions = model.predict(X_test)\n",
    "# print(predictions)\n",
    "\n",
    "accuracy = accuracy_score(y_test,predictions)\n",
    "# Print the values\n",
    "print(\"IS_HCP Model Evaluation:\")\n",
    "print(\"Accuracy is :\", accuracy)\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19a0101a",
   "metadata": {},
   "source": [
    "Txo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating mask\n",
    "# mask1 = y_train.astype(np.int64)\n",
    "# mask2 = y_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a6a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features and target arrays from the DataFrames\n",
    "X_train = df_train[txo_features].values\n",
    "y_train = df_train[txo_target].values\n",
    "# print(X_train.shape)\n",
    "\n",
    "X_test = df_test[txo_features].values\n",
    "y_test = df_test[txo_target].values\n",
    "\n",
    "# # mask = predictions == 1\n",
    "# X_train = X_train[mask1]\n",
    "# print(X_train.shape)\n",
    "\n",
    "# y_train = y_train[mask1]\n",
    "# print(y_train)\n",
    "\n",
    "# # mask = predictions == 1\n",
    "# X_test = X_test[mask2]\n",
    "# y_test = y_test[mask2]\n",
    "\n",
    "\n",
    "# y_test = y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ab371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model\n",
    "txo_model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5201ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute NaN values\n",
    "# X_train[np.isnan(X_train)] = 0\n",
    "\n",
    "# y_train = pd.Series(y_train)\n",
    "# # Fill missing values in y_train with 0\n",
    "# y_train = y_train.fillna(0)\n",
    "\n",
    "# print(X_train.shape,y_train.shape)\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "txo_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aba13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# X_test[np.isnan(X_test)] = 0\n",
    "\n",
    "t_prediction = txo_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe869b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the evaluation metrics 0.5745445036738107 24.9261978021978\n",
    "accuracy = accuracy_score(y_test,t_prediction)\n",
    "\n",
    "print(\"\\nTAXONOMY Model Evaluation:\")\n",
    "print(\"Accurcy is :\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8c5a0d0",
   "metadata": {},
   "source": [
    "Saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff081f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "joblib.dump(model, 'trained_model.pkl')\n",
    "joblib.dump(txo_model, 'txo_trained_model.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "934249b2",
   "metadata": {},
   "source": [
    "Using the models for prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e085f769",
   "metadata": {},
   "source": [
    "Part-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2bb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the trained model\n",
    "prediction_model = joblib.load('trained_model.pkl')\n",
    "\n",
    "# Load the vaex DataFrame for prediction\n",
    "df_prediction = vaex.open('Doceree-HCP-Test.hdf5')\n",
    "# Iterate over each column in the DataFrame\n",
    "for column in df_prediction.columns:\n",
    "    # Fill missing values with 0 for the current column\n",
    "    df_prediction[column].fillna(0, inplace=True)\n",
    "\n",
    "df_prediction = df_prediction.fillna(0)\n",
    "\n",
    "X_prediction = df_prediction[features].values\n",
    "user_ids = df_prediction['ID'].values\n",
    "\n",
    "# Make predictions\n",
    "predictions = prediction_model.predict(X_prediction)\n",
    "\n",
    "# Apply threshold for classification if needed\n",
    "# threshold = 0.461  //use previous used value of threshold\n",
    "\n",
    "# class_predictions = np.where(predictions >= threshold, 1, 0)\n",
    "IS_HCP = predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6aeb32af",
   "metadata": {},
   "source": [
    "Part-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea4f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Txo model\n",
    "txo_prediction_model = joblib.load('txo_trained_model.pkl')\n",
    "\n",
    "# Making dataframe for txo model\n",
    "X_prediction = X_prediction[IS_HCP == 1]\n",
    "ids = df_prediction['ID'].values\n",
    "ids = ids[IS_HCP == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_prediction = df_prediction[txo_features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9052d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction\n",
    "prediction = txo_prediction_model.predict(X_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if prediction.dtype == int:\n",
    "prediction = label_encoder.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06dec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(X_prediction,prediction)\n",
    "\n",
    "# creating mask\n",
    "mask = np.isin(user_ids,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1864b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.resize(prediction,len(mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a796fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fa0db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "txo_prediction = np.where(mask, prediction,\"\")\n",
    "\n",
    "# print(user_ids.shape,txo_prediction.shape,IS_HCP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e31d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the user IDs (userplatformuid) and the predicted HCP values and taxonomy predictions\n",
    "\n",
    "predictions = pd.DataFrame({'ID': user_ids,'taxonomy': txo_prediction, 'IS_HCP': IS_HCP})\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "predictions.to_excel('predictions.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
